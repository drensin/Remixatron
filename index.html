<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Remixatron Receiver</title>
    <!-- Google Cast Receiver SDK -->
    <script src="//www.gstatic.com/cast/sdk/libs/caf_receiver/v3/cast_receiver_framework.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html,
        body {
            width: 100%;
            height: 100%;
            background: #000000;
            overflow: hidden;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }

        #container {
            width: 100%;
            height: 100%;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
        }

        #viz-container {
            width: 100%;
            height: 100%;
            position: relative;
        }

        #canvas {
            width: 100%;
            height: 100%;
            display: block;
        }

        /* Play bar at bottom - matches desktop .floating-player */
        #play-bar {
            position: fixed;
            bottom: 24px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(30, 30, 30, 0.25);
            padding: 12px 24px;
            border-radius: 9999px;
            /* Stadium shape */
            color: #fff;
            display: flex;
            align-items: center;
            gap: 16px;
            backdrop-filter: blur(8px);
            -webkit-backdrop-filter: blur(8px);
            border: 1px solid rgba(255, 255, 255, 0.1);
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.15);
            max-width: 600px;
            width: 100%;
        }

        #thumbnail {
            width: 56px;
            height: 56px;
            border-radius: 6px;
            object-fit: cover;
            background: rgba(255, 255, 255, 0.1);
            flex-shrink: 0;
        }

        .track-info {
            flex: 1;
            min-width: 0;
            overflow: hidden;
        }

        #track-title {
            font-size: 16px;
            font-weight: 500;
            white-space: nowrap;
            overflow: hidden;
            text-overflow: ellipsis;
            margin-bottom: 4px;
        }

        #track-artist {
            font-size: 13px;
            opacity: 0.7;
            white-space: nowrap;
            overflow: hidden;
            text-overflow: ellipsis;
        }

        #sync-status {
            display: flex;
            align-items: center;
            gap: 8px;
            flex-shrink: 0;
            padding: 6px 12px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 20px;
            font-size: 12px;
        }

        .status-dot {
            width: 8px;
            height: 8px;
            border-radius: 50%;
            background: #ff4444;
            animation: pulse 1.5s infinite;
        }

        .status-dot.connected {
            background: #44ff44;
        }

        @keyframes pulse {

            0%,
            100% {
                opacity: 1;
            }

            50% {
                opacity: 0.5;
            }
        }

        /* Hidden audio element */
        #audio {
            display: none;
        }

        /* Loading overlay */
        #loading-overlay {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: rgba(10, 10, 26, 0.95);
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            z-index: 100;
            transition: opacity 0.5s ease;
        }

        #loading-overlay.hidden {
            opacity: 0;
            pointer-events: none;
        }

        .spinner {
            width: 60px;
            height: 60px;
            border: 3px solid rgba(255, 255, 255, 0.1);
            border-top-color: #00ff99;
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }

        @keyframes spin {
            to {
                transform: rotate(360deg);
            }
        }

        .loading-text {
            color: #fff;
            margin-top: 20px;
            font-size: 14px;
            opacity: 0.6;
        }
    </style>
</head>

<body>
    <div id="container">
        <div id="viz-container">
            <canvas id="canvas"></canvas>
        </div>

        <div id="play-bar">
            <img id="thumbnail" src="" alt="">
            <div class="track-info">
                <div id="track-title">Waiting for track...</div>
                <div id="track-artist"></div>
            </div>
            <div id="sync-status">
                <div class="status-dot" id="status-dot"></div>
                <span id="status-text">Connecting...</span>
            </div>
        </div>

        <!-- Audio element for streaming -->
        <audio id="audio" crossorigin="anonymous"></audio>

        <!-- Loading overlay -->
        <div id="loading-overlay">
            <div class="spinner"></div>
            <div class="loading-text">Waiting for connection...</div>
        </div>
    </div>

    <script>
        /**
         * Remixatron Web Receiver
         * 
         * Connects to the desktop app via WebSocket for visualization data
         * and plays the audio stream from the HTTP endpoint.
         */

        // Configuration
        const WS_PORT = 3030;
        const AUDIO_PATH = '/stream.mp3';
        const WS_PATH = '/viz';

        /**
         * Get the host to connect to.
         * 
         * Priority order:
         * 1. Cast CONFIG message (set by initCast when running on Chromecast)
         * 2. URL parameter ?host=<hostname>
         * 3. Current page hostname (for local serving)
         * 4. Fallback to localhost
         * 
         * @returns {string} The host to connect to for WebSocket and audio.
         */
        let castConfigHost = null;  // Set by Cast framework if running on Chromecast

        function getHost() {
            if (castConfigHost) {
                return castConfigHost;
            }
            const params = new URLSearchParams(window.location.search);
            return params.get('host') || window.location.hostname || 'localhost';
        }

        // Cast namespace for custom configuration messages
        const CAST_NAMESPACE = 'urn:x-cast:com.remixatron';

        // DOM Elements
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const audio = document.getElementById('audio');
        const statusDot = document.getElementById('status-dot');
        const statusText = document.getElementById('status-text');
        const loadingOverlay = document.getElementById('loading-overlay');
        const thumbnailImg = document.getElementById('thumbnail');
        const trackTitle = document.getElementById('track-title');
        const trackArtist = document.getElementById('track-artist');

        // Visualization State
        let beats = [];
        let segments = [];
        let waveformEnvelope = [];
        let duration = 0;
        let activeBeatIndex = -1;
        let activeSegmentIndex = -1;
        let currentSeqPos = 0;
        let currentSeqLen = 0;

        // Web Audio Streaming State (for Cast/Party mode)
        // Audio chunks arrive via WebSocket and are buffered before playback.
        let audioContext = null;         // Web Audio API context
        const audioQueue = [];           // Buffer: [{audioData, beatId, segId, seqPos, seqLen}]
        const scheduledSources = [];     // Track scheduled AudioBufferSourceNodes for cleanup
        const TARGET_BUFFER_SEC = 5.0;   // Start playback when buffer has this much audio
        let nextPlayTime = 0;            // AudioContext time for next scheduled chunk
        let isStreamingAudio = false;    // True once playback has started
        let isPausedByServer = false;    // True while playback is paused (ignore incoming audio)
        let estimatedChunkDuration = 0.5; // Estimated beat duration (updated dynamically)



        // Canvas dimensions
        let width, height, centerX, centerY, radius;

        // Color palette (matches desktop app)
        const colors = [
            '#FF0055', '#00FF99', '#00CCFF', '#FFAA00', '#CC00FF',
            '#FF3300', '#AAFF00', '#0055FF', '#FF00AA', '#00FF55'
        ];

        // WebSocket connection
        let ws = null;
        let reconnectTimer = null;

        // Cast mode state
        let isCastMode = false;      // True when running on Chromecast via Cast SDK
        let playerManager = null;    // Cast PlayerManager instance (only set in Cast mode)

        /**
         * Initialize the receiver.
         * 
         * Connection priority:
         * 1. URL ?host= parameter - explicit host override (browser testing, remote access)
         * 2. Same-origin connection - when served from Axum (local hosting)
         * 3. Cast CONFIG message - when loaded on Chromecast with no URL params
         */
        function init() {
            resizeCanvas();
            window.addEventListener('resize', resizeCanvas);

            // Clean up audio when page closes
            // Use both events for cross-browser support (pagehide is more reliable on mobile/TV)
            const cleanupOnClose = () => {
                console.log('[Cleanup] Page closing, stopping all audio');
                stopAllScheduledSources();
                if (audioContext) {
                    audioContext.close();
                }
                if (ws && ws.readyState === WebSocket.OPEN) {
                    ws.close();
                }
            };
            window.addEventListener('beforeunload', cleanupOnClose);
            window.addEventListener('pagehide', cleanupOnClose);

            // Immediately update status to show JS is running
            const loadingText = document.querySelector('.loading-text');
            if (loadingText) loadingText.textContent = 'Initializing...';

            const params = new URLSearchParams(window.location.search);
            const urlHost = params.get('host');
            const pageHost = window.location.hostname;

            console.log('[Init] pageHost:', pageHost, 'urlHost:', urlHost,
                'cast:', !!window.cast, 'framework:', !!(window.cast && window.cast.framework));

            if (urlHost) {
                // Explicit host override via URL param
                console.log('[Receiver] Host from URL param:', urlHost);
                if (loadingText) loadingText.textContent = 'URL param mode...';
                connectWebSocket();
            } else if (pageHost && !pageHost.includes('github.io') && !pageHost.includes('dkr.bio')) {
                // Page is NOT served from GitHub Pages/dkr.bio - we're in Party Mode
                // This includes localhost, LAN IPs, and Axum-served pages
                console.log('[Receiver] Party Mode - connecting to:', pageHost);
                if (loadingText) loadingText.textContent = 'Party mode...';
                connectWebSocket();
            } else if (window.cast && window.cast.framework) {
                // Cast SDK present AND loaded from GitHub Pages/dkr.bio - we're on Chromecast
                console.log('[Receiver] Cast Mode - waiting for CONFIG');
                if (loadingText) loadingText.textContent = 'Cast mode - initializing...';
                initCast();
            } else {
                // Fallback - try localhost
                console.log('[Receiver] Fallback to localhost');
                if (loadingText) loadingText.textContent = 'Fallback mode...';
                connectWebSocket();
            }

            // Start render loop
            requestAnimationFrame(renderLoop);
        }

        /**
         * Initialize the Google Cast Receiver Framework.
         * 
         * Sets up the Cast context to receive CONFIG messages from the sender
         * application. Audio is streamed via WebSocket, not PlayerManager.
         */
        function initCast() {
            try {
                console.log('[Cast] Initializing Cast Receiver Framework');
                isCastMode = true;

                const context = cast.framework.CastReceiverContext.getInstance();
                // Note: We no longer use PlayerManager for audio - it comes via WebSocket
                // We still get playerManager in case it's needed for future features
                playerManager = context.getPlayerManager();

                // Listen for custom CONFIG messages on our namespace
                context.addCustomMessageListener(CAST_NAMESPACE, (event) => {
                    console.log('[Cast] Received message:', event.data);

                    if (event.data && event.data.type === 'CONFIG' && event.data.host) {
                        castConfigHost = event.data.host;
                        console.log('[Cast] Host configured:', castConfigHost);
                        // Connect WebSocket - audio comes via binary frames, viz via JSON
                        connectWebSocket();
                    }
                });

                // Configure receiver options
                const options = new cast.framework.CastReceiverOptions();
                options.disableIdleTimeout = true;  // Keep receiver alive during playback
                options.customNamespaces = {};
                options.customNamespaces[CAST_NAMESPACE] = cast.framework.system.MessageType.JSON;

                // Handle system shutdown (Back button, Voice command, etc.)
                context.addEventListener(cast.framework.system.EventType.SHUTDOWN, (event) => {
                    console.log('[Cast] System SHUTDOWN event received', event);
                    stopAllScheduledSources(); // Kill audio immediately

                    if (audioContext) {
                        try {
                            audioContext.close();
                        } catch (e) { /* ignore */ }
                    }

                    if (ws) {
                        try {
                            ws.close();
                        } catch (e) { /* ignore */ }
                    }

                    // Explicitly stop the receiver context
                    context.stop();
                });

                // Start the Cast receiver
                context.start(options);
                console.log('[Cast] Receiver started, waiting for CONFIG message');

                // Manual key handler for "Back" button (Chromecast remote often sends Backspace or Escape)
                // This is required because disableIdleTimeout=true prevents system auto-exit.
                window.addEventListener('keydown', (e) => {
                    // Backspace (8), Escape (27), or BrowserBack
                    if (e.code === 'Backspace' || e.code === 'Escape' || e.key === 'BrowserBack') {
                        console.log('[Cast] Back button detected via keydown:', e.code);
                        stopAllScheduledSources();
                        if (ws) ws.close();
                        context.stop();
                    }
                });

                statusText.textContent = 'Cast connected - waiting for host config...';
            } catch (e) {
                console.error('[Cast] INIT ERROR:', e);
                statusText.textContent = 'Cast Error: ' + e.message;
                // Also show in loading text for visibility
                const loadingText = document.querySelector('.loading-text');
                if (loadingText) {
                    loadingText.textContent = 'Cast Error: ' + e.message;
                }
            }
        }

        /**
         * Resize canvas for HiDPI displays.
         */
        function resizeCanvas() {
            const dpr = window.devicePixelRatio || 1;
            const rect = canvas.parentElement.getBoundingClientRect();

            canvas.width = rect.width * dpr;
            canvas.height = rect.height * dpr;

            canvas.style.width = `${rect.width}px`;
            canvas.style.height = `${rect.height}px`;

            ctx.scale(dpr, dpr);
            width = rect.width;
            height = rect.height;
            radius = Math.min(width, height) / 2.2;
            centerX = width / 2;
            centerY = height / 2;
        }

        /**
         * Connect to the WebSocket endpoint.
         * Guards against duplicate connections.
         */
        function connectWebSocket() {
            // Close existing connection if any
            if (ws && (ws.readyState === WebSocket.OPEN || ws.readyState === WebSocket.CONNECTING)) {
                console.log('[WS] Closing existing connection before reconnect');
                ws.close();
            }

            const host = getHost();
            const wsUrl = `ws://${host}:${WS_PORT}${WS_PATH}`;

            statusText.textContent = 'Connecting...';
            statusDot.classList.remove('connected');

            ws = new WebSocket(wsUrl);
            ws.binaryType = 'arraybuffer';  // Receive binary frames as ArrayBuffer

            ws.onopen = () => {
                console.log('[WS] Connected');
                statusText.textContent = 'Connected - Waiting for data...';
                statusDot.classList.add('connected');

                // Initialize AudioContext on connection (requires user gesture on some browsers)
                initAudioContext();
            };

            ws.onmessage = (event) => {
                if (event.data instanceof ArrayBuffer) {
                    // Binary frame: audio update
                    handleAudioFrame(event.data);
                } else {
                    // Text frame: JSON message (init, pause, stop)
                    try {
                        const msg = JSON.parse(event.data);
                        handleMessage(msg);
                    } catch (e) {
                        console.error('[WS] Failed to parse message:', e);
                    }
                }
            };

            ws.onclose = () => {
                console.log('[WS] Disconnected');
                statusText.textContent = 'Disconnected - Reconnecting...';
                statusDot.classList.remove('connected');

                // Stop audio playback on disconnect
                stopAudioStream();

                // Reconnect after delay
                if (reconnectTimer) clearTimeout(reconnectTimer);
                reconnectTimer = setTimeout(connectWebSocket, 3000);
            };

            ws.onerror = (err) => {
                console.error('[WS] Error:', err);
            };
        }

        /**
         * Initialize the Web Audio API context.
         * Must be called from a user gesture on some browsers.
         */
        function initAudioContext() {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                console.log('[Audio] AudioContext created, state:', audioContext.state);
            }
            if (audioContext.state === 'suspended') {
                audioContext.resume().then(() => {
                    console.log('[Audio] AudioContext resumed');
                });
            }
        }

        /**
         * Handle binary audio frame from WebSocket.
         * Frame format: [beat_id:u32][seg_id:u32][seq_pos:u32][seq_len:u32][MP3 data]
         */
        function handleAudioFrame(buffer) {
            const view = new DataView(buffer);
            const beatId = view.getUint32(0, true);   // little-endian
            const segId = view.getUint32(4, true);
            const seqPos = view.getUint32(8, true);
            const seqLen = view.getUint32(12, true);
            const audioData = buffer.slice(16);

            // Ignore incoming audio while paused to prevent buffering stale data
            if (isPausedByServer) {
                return;
            }

            // Add to buffer queue
            audioQueue.push({ audioData, beatId, segId, seqPos, seqLen });

            // Check if we've reached initial buffer threshold
            if (!isStreamingAudio && getBufferDuration() >= TARGET_BUFFER_SEC) {
                console.log('[Audio] Buffer full, starting playback');
                isStreamingAudio = true;

                // Check if AudioContext needs user gesture
                // Note: If context was suspended by code (e.g. stop()), resume() works without gesture.
                // If it was suspended by browser policy (autoplay), gesture is needed.
                if (audioContext && audioContext.state === 'suspended') {
                    console.log('[Audio] Context suspended, waiting for user gesture');
                    statusText.textContent = 'Tap/Click to Play';
                    const loadingText = document.querySelector('.loading-text');
                    if (loadingText) loadingText.textContent = 'Tap anywhere to start audio';
                    loadingOverlay.classList.remove('hidden'); // Ensure visible

                    // Set up one-time click handler to resume context
                    const startAudioHandler = () => {
                        console.log('[Audio] User gesture received, resuming context');
                        audioContext.resume().then(() => {
                            console.log('[Audio] Context resumed via gesture');
                            loadingOverlay.classList.add('hidden');
                            scheduleNextChunk();
                        });
                        document.body.removeEventListener('click', startAudioHandler);
                        document.body.removeEventListener('keydown', startAudioHandler);
                    };
                    document.body.addEventListener('click', startAudioHandler);
                    document.body.addEventListener('keydown', startAudioHandler);
                } else {
                    loadingOverlay.classList.add('hidden');
                }
            }

            // Always check: If we're streaming and context is running, ensure overlay is hidden.
            // This fixes the race condition where context auto-resumes after the prompt is shown.
            if (isStreamingAudio && audioContext && audioContext.state === 'running') {
                if (!loadingOverlay.classList.contains('hidden')) {
                    console.log('[Audio] Context running, forcing overlay hidden');
                    loadingOverlay.classList.add('hidden');
                }
            }

            // Trigger scheduling (guard inside scheduleNextChunk prevents reentrancy)
            scheduleNextChunk();
        }

        /**
         * Estimate total buffered audio duration.
         */
        function getBufferDuration() {
            return audioQueue.length * estimatedChunkDuration;
        }

        // Overlap in seconds to hide MP3 encoder delay gaps
        const CHUNK_OVERLAP_SEC = 0.025;  // 25ms overlap

        // Guard to prevent concurrent scheduling
        let isSchedulingChunk = false;

        /**
         * Schedule and play the next audio chunk from the buffer.
         * Uses a guard to prevent concurrent scheduling which causes dual streams.
         */
        function scheduleNextChunk() {
            // Prevent concurrent scheduling
            if (isSchedulingChunk) {
                return;
            }

            // Only schedule if we are in streaming mode (buffer full)
            if (!isStreamingAudio) {
                return;
            }

            if (audioQueue.length === 0) {
                // No chunks to schedule - wait for more to arrive
                return;
            }

            // Check if AudioContext is ready
            if (!audioContext || audioContext.state !== 'running') {
                console.log('[Audio] Context not running, state:', audioContext?.state);
                return;
            }

            isSchedulingChunk = true;
            const chunk = audioQueue.shift();

            audioContext.decodeAudioData(chunk.audioData.slice(0))
                .then(buffer => {
                    // Create gain node for volume control
                    const gainNode = audioContext.createGain();
                    gainNode.connect(audioContext.destination);

                    const source = audioContext.createBufferSource();
                    source.buffer = buffer;
                    source.connect(gainNode);

                    // Track this source and its viz timeout for cleanup
                    const scheduledItem = { source: source, timeoutId: null };
                    scheduledSources.push(scheduledItem);

                    source.onended = () => {
                        const idx = scheduledSources.indexOf(scheduledItem);
                        if (idx !== -1) scheduledSources.splice(idx, 1);
                    };

                    // Update estimated chunk duration for buffer calculation
                    estimatedChunkDuration = buffer.duration;

                    // Schedule at precise time (with overlap to hide gaps)
                    if (nextPlayTime < audioContext.currentTime) {
                        nextPlayTime = audioContext.currentTime;
                    }

                    // Force hide overlay now that we are definitely scheduling playback
                    if (!loadingOverlay.classList.contains('hidden')) {
                        console.log('[Audio] Scheduling playback, forcing overlay hidden');
                        loadingOverlay.classList.add('hidden');
                    }

                    source.start(nextPlayTime);

                    // Update viz state when this chunk actually plays
                    const delay = Math.max(0, (nextPlayTime - audioContext.currentTime) * 1000);
                    scheduledItem.timeoutId = setTimeout(() => {
                        activeBeatIndex = chunk.beatId;
                        activeSegmentIndex = chunk.segId;
                        currentSeqPos = chunk.seqPos;
                        currentSeqLen = chunk.seqLen;
                        statusText.textContent = `Beat ${chunk.beatId}`;
                    }, delay);

                    // Advance time, but subtract overlap so next chunk starts slightly early
                    nextPlayTime += buffer.duration - CHUNK_OVERLAP_SEC;

                    // Release guard and schedule next chunk if available
                    isSchedulingChunk = false;
                    if (audioQueue.length > 0) {
                        scheduleNextChunk();
                    }
                })
                .catch(err => {
                    console.error('[Audio] Decode error:', err);
                    isSchedulingChunk = false;
                    // Try next chunk
                    if (audioQueue.length > 0) {
                        scheduleNextChunk();
                    }
                });
        }

        /**
         * Stop all scheduled audio sources and clear pending viz updates.
         * Called on pause/stop to cancel already-scheduled playback.
         */
        function stopAllScheduledSources() {
            while (scheduledSources.length > 0) {
                const item = scheduledSources.pop();
                try {
                    item.source.stop();
                } catch (e) {
                    // Ignore errors (source may have already ended)
                }

                // key fix: clear the associated visualization timeout
                if (item.timeoutId !== null) {
                    clearTimeout(item.timeoutId);
                }
            }
        }

        /**
         * Stop audio streaming and clear buffer.
         */
        function stopAudioStream() {
            stopAllScheduledSources();
            audioQueue.length = 0;
            isStreamingAudio = false;
            isSchedulingChunk = false;
            nextPlayTime = 0;
            if (audioContext && audioContext.state === 'running') {
                audioContext.suspend();
            }
        }

        /**
         * Pause audio playback.
         */
        function pauseAudioStream() {
            isPausedByServer = true;  // Stop accepting new audio frames
            stopAllScheduledSources();  // Cancel already-scheduled audio
            if (audioContext) {
                audioContext.suspend();
                audioQueue.length = 0;
                isStreamingAudio = false; // Force re-buffering on resume
                isSchedulingChunk = false;
                console.log('[Audio] Paused, scheduled sources stopped, buffer cleared');
            }
        }

        /**
         * Resume audio playback.
         */
        function resumeAudioStream() {
            isPausedByServer = false;  // Start accepting audio frames again
            if (audioContext) {
                // Show buffering feedback
                if (statusText) statusText.textContent = "Buffering...";
                console.log('[Audio] Resuming, queue:', audioQueue.length, 'streaming:', isStreamingAudio, 'ctx state:', audioContext.state);

                // Clear any stale buffered chunks (shouldn't be any since we ignored during pause)
                if (audioQueue.length > 0) {
                    console.log('[Audio] Clearing', audioQueue.length, 'stale buffered chunks');
                    audioQueue.length = 0;
                }

                audioContext.resume().then(() => {
                    console.log('[Audio] Context resumed, currentTime:', audioContext.currentTime);
                    // Reset next play time to current context time
                    nextPlayTime = audioContext.currentTime;
                });
            }
        }

        /**
         * Handle incoming WebSocket messages (JSON only - init, pause, stop).
         * Audio updates come via binary frames handled by handleAudioFrame().
         */
        function handleMessage(msg) {
            switch (msg.type) {
                case 'init':
                    // Initialize with full visualization data
                    beats = msg.beats || [];
                    segments = msg.segments || [];
                    waveformEnvelope = msg.waveform || [];

                    if (beats.length > 0) {
                        const lastBeat = beats[beats.length - 1];
                        duration = lastBeat.start + lastBeat.duration;
                    }

                    // Update play bar with track metadata
                    trackTitle.textContent = msg.title || 'Unknown Title';
                    trackArtist.textContent = msg.artist || 'Unknown Artist';
                    if (msg.thumbnail) {
                        thumbnailImg.src = msg.thumbnail;
                        thumbnailImg.style.display = 'block';
                    } else {
                        thumbnailImg.style.display = 'none';
                    }

                    // Reset audio streaming state for new track
                    stopAudioStream();

                    console.log(`[Viz] Loaded: ${beats.length} beats, ${segments.length} segments, title: "${msg.title}"`);
                    statusText.textContent = 'Buffering audio...';
                    // Note: loadingOverlay is hidden when buffer fills (in handleAudioFrame)
                    break;

                case 'update':
                    // Check for shutdown signal (from Desktop Stop Casting)
                    if (msg.shutdown) {
                        console.log('[Control] Shutdown received');
                        stopAudioStream();

                        // If running as Cast Receiver, explicit stop
                        if (isCastMode && window.cast && window.cast.framework) {
                            console.log('[Cast] Stopping Cast Receiver Context');
                            const context = window.cast.framework.CastReceiverContext.getInstance();
                            context.stop();
                        }

                        // Close window (works for both Cast and simple popups)
                        window.close();
                        break;
                    }

                    // Check for stop signal (Playback Stop)
                    if (msg.stopped) {
                        console.log('[Control] Stop received');
                        stopAudioStream();
                        activeBeatIndex = -1;
                        activeSegmentIndex = -1;
                        loadingOverlay.classList.remove('hidden');
                        document.querySelector('.loading-text').textContent = 'Waiting for playback...';
                        break;
                    }

                    // Check for pause signal
                    if (msg.paused !== undefined) {
                        if (msg.paused) {
                            console.log('[Control] Pause received');
                            pauseAudioStream();
                            statusText.textContent = 'Paused';
                        } else {
                            console.log('[Control] Resume received');
                            resumeAudioStream();
                        }
                        break;
                    }

                    // Note: Normal beat updates now come via binary frames, not JSON
                    break;

                default:
                    console.warn('[WS] Unknown message type:', msg.type);
            }
        }

        // NOTE: Old playback functions (stopPlayback, pausePlayback, resumePlayback,
        // startAudioCast, startAudio) have been removed. All audio playback is now
        // handled by the Web Audio streaming functions: handleAudioFrame, scheduleNextChunk,
        // pauseAudioStream, resumeAudioStream, stopAudioStream.

        /**
         * Convert time to angle (radians).
         */
        function getAngle(time) {
            return (time / duration) * 2 * Math.PI - (Math.PI / 2);
        }

        /**
         * Main render loop.
         * Uses dirty-flag rendering - only redraws when beat changes.
         * This drastically reduces CPU usage on low-powered devices like TVs.
         */
        let lastRenderedBeatIndex = -1;
        let needsRedraw = true;  // Force initial draw

        function renderLoop() {
            // Only redraw if beat changed or forced
            if (beats.length > 0 && (needsRedraw || activeBeatIndex !== lastRenderedBeatIndex)) {
                draw();
                lastRenderedBeatIndex = activeBeatIndex;
                needsRedraw = false;
            }
            requestAnimationFrame(renderLoop);
        }

        // Force redraw when segment changes (called from scheduleNextChunk)
        function markNeedsRedraw() {
            needsRedraw = true;
        }

        /**
         * Draw the visualization.
         */
        function draw() {
            ctx.clearRect(0, 0, width, height);
            ctx.lineCap = 'butt';

            // 1. Draw Waveform Ring (inner)
            drawWaveformRing();

            // 2. Draw Segment Ring (outer)
            ctx.lineWidth = 15;
            segments.forEach((seg, i) => {
                const startAngle = getAngle(seg.start_time);
                const endAngle = getAngle(seg.end_time);

                ctx.beginPath();
                ctx.arc(centerX, centerY, radius, startAngle, endAngle);
                ctx.strokeStyle = colors[seg.label % colors.length];
                ctx.globalAlpha = (activeSegmentIndex === -1 || activeSegmentIndex === i) ? 1.0 : 0.3;
                ctx.stroke();
                ctx.globalAlpha = 1.0;
            });

            // 3. Draw Beat Cursor
            if (activeBeatIndex !== -1 && activeBeatIndex < beats.length) {
                const beat = beats[activeBeatIndex];
                const angle = getAngle(beat.start);

                const x = centerX + Math.cos(angle) * radius;
                const y = centerY + Math.sin(angle) * radius;

                // Glow effect
                ctx.shadowBlur = 15;
                ctx.shadowColor = '#FFFFFF';

                ctx.beginPath();
                ctx.arc(x, y, 8, 0, 2 * Math.PI);
                ctx.fillStyle = '#FFFFFF';
                ctx.fill();

                ctx.shadowBlur = 0;

                // 4. Draw Jump Arcs
                ctx.lineWidth = 1.5;
                beat.jump_candidates.forEach(targetIdx => {
                    if (targetIdx >= beats.length) return;

                    const targetBeat = beats[targetIdx];
                    const targetAngle = getAngle(targetBeat.start);

                    const tx = centerX + Math.cos(targetAngle) * (radius - 20);
                    const ty = centerY + Math.sin(targetAngle) * (radius - 20);
                    const sx = centerX + Math.cos(angle) * (radius - 20);
                    const sy = centerY + Math.sin(angle) * (radius - 20);

                    const targetSeg = segments[targetBeat.segment];
                    const targetColor = targetSeg ? colors[targetSeg.label % colors.length] : '#FFFFFF';

                    ctx.beginPath();
                    ctx.moveTo(sx, sy);
                    ctx.quadraticCurveTo(centerX, centerY, tx, ty);
                    ctx.strokeStyle = targetColor;
                    ctx.globalAlpha = 0.5;
                    ctx.stroke();
                    ctx.globalAlpha = 1.0;
                });
            }

            // 5. Draw Countdown Ring
            if (activeBeatIndex !== -1 && currentSeqLen > 0) {
                drawCountdown();
            }
        }

        /**
         * Draw the waveform amplitude ring.
         */
        function drawWaveformRing() {
            if (waveformEnvelope.length === 0 || segments.length === 0) return;

            const segmentRingWidth = 15;
            const waveformRingWidth = segmentRingWidth * 1.5;
            const baselineRadius = radius - segmentRingWidth / 2 - 3 - waveformRingWidth / 2;
            const numBars = waveformEnvelope.length;
            const barWidth = 1.5;

            for (let i = 0; i < numBars; i++) {
                const normalizedPos = i / numBars;
                const angle = normalizedPos * 2 * Math.PI - (Math.PI / 2);
                const time = normalizedPos * duration;

                let amp = waveformEnvelope[i] || 0;
                amp = Math.max(amp, 0.1);

                const barHalfHeight = (waveformRingWidth / 2) * amp;
                const innerRadius = baselineRadius - barHalfHeight;
                const outerRadius = baselineRadius + barHalfHeight;

                // Find segment for this time
                let segIdx = -1;
                for (let s = 0; s < segments.length; s++) {
                    if (time >= segments[s].start_time && time < segments[s].end_time) {
                        segIdx = s;
                        break;
                    }
                }

                const isActive = (segIdx !== -1) && (activeSegmentIndex === -1 || activeSegmentIndex === segIdx);

                const innerX = centerX + Math.cos(angle) * innerRadius;
                const innerY = centerY + Math.sin(angle) * innerRadius;
                const outerX = centerX + Math.cos(angle) * outerRadius;
                const outerY = centerY + Math.sin(angle) * outerRadius;

                ctx.beginPath();
                ctx.moveTo(innerX, innerY);
                ctx.lineTo(outerX, outerY);
                ctx.lineWidth = barWidth;
                ctx.lineCap = 'round';

                if (isActive) {
                    ctx.shadowBlur = 6;
                    ctx.shadowColor = '#FFFFFF';
                    ctx.globalAlpha = 1.0;
                    ctx.strokeStyle = '#FFFFFF';
                } else {
                    ctx.shadowBlur = 0;
                    ctx.globalAlpha = 0.35;
                    ctx.strokeStyle = '#AAAAAA';
                }

                ctx.stroke();
            }

            ctx.shadowBlur = 0;
            ctx.globalAlpha = 1.0;
            ctx.lineCap = 'butt';
        }

        /**
         * Draw the countdown pulse ring.
         */
        function drawCountdown() {
            const remaining = currentSeqLen - currentSeqPos;
            const pct = currentSeqPos / currentSeqLen;

            const cx = centerX;
            const cy = centerY;
            const innerRadius = 28;
            const bgRadius = innerRadius + 5;

            // Determine pulse color
            let color = '#00FF99'; // Green
            if (remaining <= 2) {
                color = '#FF0055'; // Red
            } else if (remaining <= 4) {
                color = '#FFAA00'; // Orange
            }

            // Background track
            ctx.beginPath();
            ctx.arc(cx, cy, bgRadius, 0, 2 * Math.PI);
            ctx.strokeStyle = 'rgba(255, 255, 255, 0.1)';
            ctx.lineWidth = 4;
            ctx.lineCap = 'round';
            ctx.stroke();

            // Progress arc
            const startAngle = -Math.PI / 2;
            const endAngle = startAngle + pct * 2 * Math.PI;
            ctx.beginPath();
            ctx.arc(cx, cy, bgRadius, startAngle, endAngle);
            ctx.strokeStyle = color;
            ctx.lineWidth = 4;
            ctx.lineCap = 'round';
            ctx.stroke();

            // Number
            ctx.textAlign = 'center';
            ctx.textBaseline = 'middle';
            ctx.fillStyle = color;
            ctx.font = 'bold 24px sans-serif';
            ctx.fillText(remaining.toString(), cx, cy - 2);

            // Label
            ctx.font = '8px sans-serif';
            ctx.fillStyle = 'rgba(255, 255, 255, 0.6)';
            ctx.fillText('BRANCH', cx, cy + 12);
        }

        // Initialize on load
        window.addEventListener('DOMContentLoaded', init);
    </script>
</body>

</html>